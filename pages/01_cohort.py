"""ã‚³ãƒ›ãƒ¼ãƒˆåˆ†æãƒšãƒ¼ã‚¸ - ç¶™ç¶šç‡ãƒ»æ®‹å­˜ç‡ãƒ»LTVãƒ»ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡."""

from __future__ import annotations

from datetime import date

import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import streamlit as st

from src.bigquery_client import execute_query, fetch_filtered_options, get_bigquery_client
from src.components.cohort_heatmap import render_cohort_heatmap, render_retention_line_chart
from src.components.download_button import render_download_buttons
from src.components.filters import render_cohort_filters
from src.components.metrics_row import render_metrics
from src.config_loader import get_product_cycle, get_upsell_target, get_upsell_targets, load_upsell_mappings
from src.constants import Col
from src.queries.common import get_table_ref
from src.queries.cohort import (
    build_aggregate_cohort_sql,
    build_cohort_sql,
    build_drilldown_sql,
    build_max_date_sql,
    build_upsell_rate_monthly_sql,
    build_upsell_rate_sql,
    build_upsell_sql,
)
from src.session import SessionKey, get_selected_company_key
from src.transforms.cohort_transform import (
    build_1year_ltv_table,
    build_aggregate_table,
    build_dimension_summary_table,
    build_drilldown_rate_matrices,
    build_drilldown_retention_table,
    build_product_summary_table,
    build_retention_rate_matrix,
    build_retention_table,
    build_shipping_schedule,
    compute_aggregate_metrics,
    compute_max_orders_in_period,
    compute_summary_metrics,
    compute_upsell_rate,
)


# =====================================================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼: è‰²ä»˜ãHTMLãƒ†ãƒ¼ãƒ–ãƒ«
# =====================================================================
def _styled_table(df: pd.DataFrame, value_col: str, color: str = "blue") -> str:
    """å€¤ã®å¤§ãã•ã«å¿œã˜ã¦è‰²ã®æ¿ƒã•ãŒå¤‰ã‚ã‚‹HTMLãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç”Ÿæˆ."""
    if color == "blue":
        bg = "rgba(74, 144, 217, {alpha})"
    elif color == "green":
        bg = "rgba(52, 211, 153, {alpha})"
    else:
        bg = "rgba(74, 144, 217, {alpha})"

    max_val = df[value_col].max() if len(df) > 0 else 100

    rows_html = ""
    for _, row in df.iterrows():
        val = row[value_col]
        alpha = round(val / max_val * 0.6 + 0.05, 2) if max_val > 0 else 0.05
        bg_color = bg.format(alpha=alpha)
        text_color = "#1a1a2e" if alpha < 0.4 else "#ffffff"

        cells = ""
        for col_name in df.columns:
            v = row[col_name]
            if col_name == value_col:
                cells += f'<td style="background:{bg_color};color:{text_color};font-weight:600;text-align:right;padding:4px 8px;">{v}%</td>'
            elif isinstance(v, (int, float)) and col_name != df.columns[0]:
                cells += f'<td style="text-align:right;padding:4px 8px;">{int(v):,}</td>'
            else:
                cells += f'<td style="padding:4px 8px;">{v}</td>'
        rows_html += f"<tr>{cells}</tr>"

    header = "".join(
        f'<th style="padding:4px 8px;text-align:center;border-bottom:2px solid #ddd;">{c}</th>'
        for c in df.columns
    )

    return f"""
    <div style="max-height:460px;overflow-y:auto;">
    <table style="width:100%;border-collapse:collapse;font-size:13px;">
    <thead><tr>{header}</tr></thead>
    <tbody>{rows_html}</tbody>
    </table>
    </div>
    """


# =====================================================================
# ãƒ˜ãƒ«ãƒ‘ãƒ¼: ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡è¡¨ç¤º
# =====================================================================
def _upsell_label_html(title: str, before_name: str, after_name: str) -> str:
    """ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡ã®2æ®µãƒ©ãƒ™ãƒ«HTMLã‚’ç”Ÿæˆ."""
    return (
        f"**{title}**\n\n"
        f"USå‰ï¼š{before_name}  \n"
        f"USå¾Œï¼š{after_name}"
    )


def _render_upsell_pair(
    client,
    company_key: str,
    normal_name: str,
    upsell_name: str,
    label_title: str,
    date_from_str: str | None,
    date_to_str: str | None,
    *,
    skip_if_no_normal: bool = False,
    pair_key: str = "",
):
    """1çµ„ã®ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡ã‚’è¡¨ç¤ºï¼ˆåˆå›åˆ¤å®šã®ã¿ï¼‰ã€‚skipæ™‚ã¯UIè‡ªä½“ã‚’å‡ºã•ãªã„ã€‚"""
    # skip_if_no_normal ã®å ´åˆã€ã¾ãšãƒ‡ãƒ¼ã‚¿æœ‰ç„¡ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆæç”»
    if skip_if_no_normal:
        sql_check = build_upsell_rate_sql(
            company_key, normal_name, upsell_name,
            date_from_str, date_to_str,
        )
        try:
            df_check = execute_query(client, sql_check)
            if df_check.empty or df_check["upsell_rate"].iloc[0] is None:
                return
            if int(df_check.iloc[0]["normal_count"]) == 0:
                return
        except Exception:
            return

    # ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã¨ã—ã¦æç”»ï¼ˆæ—¥ä»˜å¤‰æ›´æ™‚ã«ã“ã“ã ã‘å†å®Ÿè¡Œï¼‰
    _upsell_pair_fragment(
        client, company_key, normal_name, upsell_name,
        label_title, date_from_str, date_to_str,
        pair_key=pair_key,
    )


@st.fragment
def _upsell_pair_fragment(
    client,
    company_key: str,
    normal_name: str,
    upsell_name: str,
    label_title: str,
    date_from_str: str | None,
    date_to_str: str | None,
    *,
    pair_key: str = "",
):
    """ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ã•ã‚ŒãŸã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡è¡¨ç¤ºã€‚æ—¥ä»˜å¤‰æ›´æ™‚ã«ã“ã®éƒ¨åˆ†ã ã‘å†å®Ÿè¡Œã€‚"""
    _key_base = pair_key or f"{normal_name}_{upsell_name}"
    _k_from = f"us_period_from_{_key_base}"
    _k_to = f"us_period_to_{_key_base}"

    # session_state ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šæ—¥ä»˜ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†ã€ãªã‘ã‚Œã°è‡ªå‹•æ¤œå‡º
    has_override = _k_from in st.session_state
    if has_override:
        override_from = st.session_state[_k_from].strftime("%Y-%m-%d")
        override_to = st.session_state[_k_to].strftime("%Y-%m-%d")
        query_from = override_from
        query_to = override_to
    else:
        query_from = date_from_str
        query_to = date_to_str

    sql = build_upsell_rate_sql(
        company_key, normal_name, upsell_name,
        query_from, query_to,
    )
    try:
        df = execute_query(client, sql)
        if df.empty or df["upsell_rate"].iloc[0] is None:
            st.markdown(_upsell_label_html(label_title, normal_name, upsell_name))
            st.caption("ãƒ‡ãƒ¼ã‚¿ãªã—")
            return
        row = df.iloc[0]
        rate = round(float(row["upsell_rate"]), 1)
        normal_count = int(row["normal_count"])
        upsell_count = int(row["upsell_count"])
        period_start = str(row["period_start"])[:10]
        period_end = str(row["period_end"])[:10]

        st.markdown(_upsell_label_html(label_title, normal_name, upsell_name))

        # å¯¾è±¡æœŸé–“ã‚’ date_input ã§è¡¨ç¤ºï¼ˆåˆå›ã¯è‡ªå‹•æ¤œå‡ºå€¤ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ï¼‰
        if not has_override:
            st.session_state[_k_from] = date.fromisoformat(period_start)
            st.session_state[_k_to] = date.fromisoformat(period_end)

        dcols = st.columns([1, 1])
        with dcols[0]:
            st.date_input("å¯¾è±¡é–‹å§‹æ—¥", key=_k_from)
        with dcols[1]:
            st.date_input("å¯¾è±¡çµ‚äº†æ—¥", key=_k_to)

        st.metric("", f"{rate}%")
        st.caption(f"é€šå¸¸: {normal_count:,}äºº / ã‚¢ãƒƒãƒ—ã‚»ãƒ«: {upsell_count:,}äºº")
    except Exception as e:
        st.markdown(_upsell_label_html(label_title, normal_name, upsell_name))
        st.caption(f"ã‚¨ãƒ©ãƒ¼ ({e})")


def _render_upsell_monthly(
    client,
    company_key: str,
    normal_name: str,
    upsell_name: str,
    label_title: str,
    date_from_str: str | None,
    date_to_str: str | None,
    *,
    skip_if_no_normal: bool = False,
):
    """æœˆåˆ¥ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡ãƒ†ãƒ¼ãƒ–ãƒ«+ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º."""
    sql = build_upsell_rate_monthly_sql(
        company_key, normal_name, upsell_name,
        date_from_str, date_to_str,
    )
    label_md = _upsell_label_html(label_title, normal_name, upsell_name)
    try:
        df = execute_query(client, sql)
        if df.empty:
            if not skip_if_no_normal:
                st.markdown(label_md)
                st.info("ãƒ‡ãƒ¼ã‚¿ãªã—")
            return

        # é€šå¸¸å•†å“ãŒå…¨æœˆã§0äººãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
        if skip_if_no_normal and df["normal_count"].sum() == 0:
            return

        display_df = df[["cohort_month", "normal_count", "upsell_count", "upsell_rate"]].copy()
        display_df.columns = ["æœˆ", "é€šå¸¸å•†å“(äºº)", "ã‚¢ãƒƒãƒ—ã‚»ãƒ«å•†å“(äºº)", "ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡(%)"]
        display_df["ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡(%)"] = display_df["ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡(%)"].round(1)

        st.markdown(label_md)
        st.dataframe(display_df, use_container_width=True, hide_index=True)

        # æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•
        if len(display_df) > 1:
            fig = go.Figure()
            fig.add_trace(go.Scatter(
                x=display_df["æœˆ"],
                y=display_df["ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡(%)"],
                mode="lines+markers+text",
                text=display_df["ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡(%)"].apply(lambda v: f"{v}%"),
                textposition="top center",
                textfont=dict(size=9),
                line=dict(color="#E74C3C", width=2),
                marker=dict(size=6),
            ))
            fig.update_layout(
                title=f"{label_title} æ¨ç§»",
                xaxis_title="æœˆ",
                yaxis_title="ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡ (%)",
                height=350,
                margin=dict(l=50, r=30, t=40, b=30),
            )
            st.plotly_chart(fig, use_container_width=True)
    except Exception as e:
        st.markdown(label_md)
        st.error(f"ã‚¨ãƒ©ãƒ¼ ({e})")


# =====================================================================
# ãƒšãƒ¼ã‚¸åˆæœŸåŒ–
# =====================================================================
st.header("ã‚³ãƒ›ãƒ¼ãƒˆåˆ†æ")

company_key = get_selected_company_key()
if not company_key:
    st.warning("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ä¼šç¤¾ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    st.stop()

date_from = st.session_state.get(SessionKey.DATE_FROM)
date_to = st.session_state.get(SessionKey.DATE_TO)

with st.sidebar:
    filters = render_cohort_filters(company_key)

client = get_bigquery_client()
drilldown_col = filters["drilldown_column"]

date_from_str = date_from.strftime("%Y-%m-%d") if date_from else None
date_to_str = date_to.strftime("%Y-%m-%d") if date_to else None

filter_params = dict(
    company_key=company_key,
    date_from=date_from_str,
    date_to=date_to_str,
    product_categories=filters["product_categories"],
    ad_groups=filters["ad_groups"],
    product_names=filters["product_names"],
)

# ãƒ‡ãƒ¼ã‚¿æœ€çµ‚æ—¥ã‚’å–å¾—
try:
    max_date_df = execute_query(client, build_max_date_sql(company_key))
    if not max_date_df.empty and max_date_df["max_date"].iloc[0] is not None:
        raw_val = max_date_df["max_date"].iloc[0]
        if isinstance(raw_val, date):
            data_cutoff_date = raw_val
        elif hasattr(raw_val, "date"):
            data_cutoff_date = raw_val.date()
        else:
            data_cutoff_date = date.today()
    else:
        data_cutoff_date = date.today()
except Exception as e:
    st.warning(f"ãƒ‡ãƒ¼ã‚¿ã‚«ãƒƒãƒˆã‚ªãƒ•æ—¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    data_cutoff_date = date.today()


# =====================================================================
# ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–: ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ / LTV / æœˆåˆ¥ / ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡
# =====================================================================
main_tab_drilldown, main_tab_aggregate, main_tab_monthly, main_tab_upsell = st.tabs(
    ["ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³", "LTV", "æœˆåˆ¥", "ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡"]
)


# =====================================================================
# ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ã‚¿ãƒ– (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: å®šæœŸå•†å“ååˆ¥)
# =====================================================================
with main_tab_drilldown:
    dd_col = drilldown_col  # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§é¸æŠã•ã‚ŒãŸãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³è»¸

    if dd_col is None:
        st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³è»¸ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    elif not st.button("è¡¨ç¤ºã™ã‚‹", key="btn_drilldown", type="primary"):
        st.info("ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¨­å®šã—ã¦ã€Œè¡¨ç¤ºã™ã‚‹ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    else:
        dd_sql = build_drilldown_sql(drilldown_column=dd_col, **filter_params)
        try:
            dd_df = execute_query(client, dd_sql)
        except Exception as e:
            st.error(f"BigQueryã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            st.stop()

        if dd_df.empty:
            st.info("è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            dimension_values = sorted(dd_df["dimension_col"].unique())

            # ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³è»¸ã®ãƒ©ãƒ™ãƒ«
            dd_label_map = {
                Col.SUBSCRIPTION_PRODUCT_NAME: "å®šæœŸå•†å“å",
                Col.AD_GROUP: "åºƒå‘Šã‚°ãƒ«ãƒ¼ãƒ—",
                Col.PRODUCT_CATEGORY: "å•†å“ã‚«ãƒ†ã‚´ãƒª",
            }
            dd_axis_label = dd_label_map.get(dd_col, "ã‚°ãƒ«ãƒ¼ãƒ—")
            st.info(f"**{dd_axis_label}åˆ¥**: {len(dimension_values)} ä»¶")
            st.caption(f"ãƒ‡ãƒ¼ã‚¿ã‚«ãƒƒãƒˆã‚ªãƒ•æ—¥: {data_cutoff_date}")

            # ---------- å®šæœŸå•†å“å åˆ¥ ----------
            if dd_col == Col.SUBSCRIPTION_PRODUCT_NAME:
                dd_sub_retention, dd_sub_upsell = st.tabs(["ç¶™ç¶šç‡", "ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡"])

                with dd_sub_retention:
                    for pname in dimension_values:
                        with st.expander(f"{pname}", expanded=False):
                            summary = build_product_summary_table(dd_df, pname, data_cutoff_date)
                            if summary.empty:
                                st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                                continue
                            st.dataframe(summary, use_container_width=True, hide_index=True)

                with dd_sub_upsell:
                    has_any_mapping = False
                    for pname in dimension_values:
                        targets = get_upsell_targets(pname)
                        if not targets:
                            continue
                        has_any_mapping = True
                        with st.expander(f"{pname}", expanded=False):
                            # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–: upsell_names ã¨ upsell_upsell_names ã‚’é›†ç´„
                            _dd_upsell_names = []
                            _dd_upsell_upsell_names = []
                            for t in targets:
                                un = t.get("upsell_name", "")
                                uun = t.get("upsell_upsell_name")
                                if un and un not in _dd_upsell_names:
                                    _dd_upsell_names.append(un)
                                if uun and uun not in _dd_upsell_upsell_names:
                                    _dd_upsell_upsell_names.append(uun)

                            # ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡
                            for _ui, un in enumerate(_dd_upsell_names):
                                _render_upsell_pair(
                                    client, company_key,
                                    pname, un,
                                    "ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡",
                                    date_from_str, date_to_str,
                                    pair_key=f"dd_{pname[:10]}_{_ui}",
                                )

                            # ã‚¢ãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡: å„ upsell Ã— å„ upsell_upsell
                            if _dd_upsell_upsell_names:
                                st.divider()
                                for _uui, uun in enumerate(_dd_upsell_upsell_names):
                                    for _ui2, un in enumerate(_dd_upsell_names):
                                        _render_upsell_pair(
                                            client, company_key,
                                            un, uun,
                                            "ï½±ï½¯ï¾Œï¾Ÿï½±ï½¯ï¾Œï¾Ÿï½¾ï¾™ç‡",
                                            date_from_str, date_to_str,
                                            skip_if_no_normal=True,
                                            pair_key=f"dd_uu_{pname[:10]}_{_uui}_{_ui2}",
                                        )
                    if not has_any_mapping:
                        st.info("ã‚¢ãƒƒãƒ—ã‚»ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å•†å“ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒã‚¹ã‚¿ç®¡ç†ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")

            # ---------- åºƒå‘Šã‚°ãƒ«ãƒ¼ãƒ— åˆ¥ ----------
            elif dd_col == Col.AD_GROUP:
                for grp_name in dimension_values:
                    with st.expander(f"{grp_name}", expanded=False):
                        summary = build_dimension_summary_table(dd_df, grp_name)
                        if summary.empty:
                            st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                            continue
                        st.dataframe(summary, use_container_width=True, hide_index=True)

            # ---------- å•†å“ã‚«ãƒ†ã‚´ãƒª åˆ¥ ----------
            elif dd_col == Col.PRODUCT_CATEGORY:
                for cat_name in dimension_values:
                    with st.expander(f"ã‚«ãƒ†ã‚´ãƒª: {cat_name}", expanded=False):
                        summary = build_dimension_summary_table(dd_df, cat_name)
                        if summary.empty:
                            st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                            continue
                        st.dataframe(summary, use_container_width=True, hide_index=True)


# =====================================================================
# é€šç®—ã‚¿ãƒ– â€” æ®‹å­˜ç‡ãƒ»ç¶™ç¶šç‡ãƒ»1å¹´LTV
# =====================================================================
with main_tab_aggregate:
    if not filters["product_names"]:
        st.info("æ­£ç¢ºãªãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºã®ãŸã‚ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€Œå®šæœŸå•†å“åã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    elif not st.button("è¡¨ç¤ºã™ã‚‹", key="btn_aggregate", type="primary"):
        st.info("ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¨­å®šã—ã¦ã€Œè¡¨ç¤ºã™ã‚‹ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    else:
        agg_sql = build_aggregate_cohort_sql(**filter_params)
        try:
            agg_df = execute_query(client, agg_sql)
        except Exception as e:
            st.error(f"BigQueryã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            agg_df = pd.DataFrame()

        if agg_df.empty:
            st.info("è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            _agg_pnames = filters.get("product_names")
            agg_metrics = compute_aggregate_metrics(agg_df)

            # å•†å“å1ã¤é¸æŠæ™‚: ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ãƒ‡ãƒ¼ã‚¿ã§ãƒã‚¹ã‚¯ä»˜ãåˆç®—
            _agg_dd_df = None
            _agg_pname = None
            if _agg_pnames and len(_agg_pnames) == 1:
                _agg_pname = _agg_pnames[0]
                try:
                    _agg_dd_sql = build_drilldown_sql(
                        drilldown_column=Col.SUBSCRIPTION_PRODUCT_NAME,
                        **filter_params,
                    )
                    _agg_dd_df = execute_query(client, _agg_dd_sql)
                except Exception:
                    _agg_dd_df = None

            agg_table = build_aggregate_table(
                agg_df,
                drilldown_df=_agg_dd_df,
                product_name=_agg_pname,
                data_cutoff_date=data_cutoff_date,
            )

            if agg_table.empty:
                st.info("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            else:
                # 1å¹´LTVè¨ˆç®—
                selected_pnames = filters.get("product_names")
                if selected_pnames and len(selected_pnames) == 1:
                    cycle1, cycle2 = get_product_cycle(selected_pnames[0])
                else:
                    cycle1, cycle2 = 30, 30

                proj_rates = st.session_state.get("proj_rates", {})
                proj_amounts = st.session_state.get("proj_amounts", {})

                ltv_table = build_1year_ltv_table(
                    agg_df, cycle1, cycle2,
                    projected_rates=proj_rates or None,
                    projected_amounts=proj_amounts or None,
                    filtered_agg_table=agg_table,
                )

                # ========== KPIã‚«ãƒ¼ãƒ‰ ==========
                kpi1, kpi2, kpi3, kpi4 = st.columns(4)
                kpi1.metric("æ–°è¦é¡§å®¢æ•°", f"{agg_metrics['total_new_users']:,}")
                kpi2.metric("2å›ç›®æ®‹å­˜ç‡", f"{agg_metrics['retention_2']}%")

                r6 = agg_table.loc[agg_table["å®šæœŸå›æ•°"] == "6å›ç›®", "æ®‹å­˜ç‡(%)"]
                kpi3.metric("6å›ç›®æ®‹å­˜ç‡", f"{r6.values[0]}%" if len(r6) > 0 else "-")

                if not ltv_table.empty:
                    year_ltv = ltv_table["LTV(å††)"].iloc[-1]
                    kpi4.metric("1å¹´LTV", f"Â¥{year_ltv:,}")
                else:
                    kpi4.metric("1å¹´LTV", "-")

                st.markdown("")

                # ========== ãƒ¡ã‚¤ãƒ³3ã‚«ãƒ©ãƒ  ==========
                col_surv, col_cont, col_ltv = st.columns(3)

                with col_surv:
                    st.markdown("##### æ®‹å­˜ç‡")
                    surv_df = agg_table[["å®šæœŸå›æ•°", "ç¶™ç¶šäººæ•°", "æ®‹å­˜ç‡(%)"]].copy()
                    surv_df.columns = ["å›æ•°", "äººæ•°", "æ®‹å­˜ç‡(%)"]
                    html = _styled_table(surv_df, value_col="æ®‹å­˜ç‡(%)", color="blue")
                    st.markdown(html, unsafe_allow_html=True)

                with col_cont:
                    st.markdown("##### ç¶™ç¶šç‡ (å‰å›æ¯”)")
                    cont_df = agg_table[["å®šæœŸå›æ•°", "ç¶™ç¶šäººæ•°", "ç¶™ç¶šç‡(%)"]].copy()
                    cont_df.columns = ["å›æ•°", "äººæ•°", "ç¶™ç¶šç‡(%)"]
                    html = _styled_table(cont_df, value_col="ç¶™ç¶šç‡(%)", color="green")
                    st.markdown(html, unsafe_allow_html=True)

                with col_ltv:
                    st.markdown("##### 1å¹´LTV")
                    if not ltv_table.empty:
                        display_ltv = ltv_table[["å®šæœŸå›æ•°", "å¹³å‡å˜ä¾¡(å††)", "LTV(å††)", "äºˆæ¸¬"]].copy()
                        display_ltv["å¹³å‡å˜ä¾¡(å††)"] = display_ltv["å¹³å‡å˜ä¾¡(å††)"].apply(lambda v: f"Â¥{v:,}")
                        display_ltv["LTV(å††)"] = display_ltv["LTV(å††)"].apply(lambda v: f"Â¥{v:,}")
                        display_ltv["äºˆæ¸¬"] = display_ltv["äºˆæ¸¬"].apply(lambda v: "äºˆæ¸¬" if v else "å®Ÿç¸¾")
                        st.dataframe(display_ltv, use_container_width=True, hide_index=True, height=460)

                # ========== äºˆæ¸¬å€¤ã®ç·¨é›† ==========
                if not ltv_table.empty and ltv_table["äºˆæ¸¬"].any():
                    st.markdown("---")
                    st.markdown("##### äºˆæ¸¬å€¤ã®ç·¨é›†")
                    st.caption("äºˆæ¸¬è¡Œã®ç¶™ç¶šç‡ãƒ»å¹³å‡å˜ä¾¡ã‚’ç·¨é›†ã™ã‚‹ã¨1å¹´LTVãŒå†è¨ˆç®—ã•ã‚Œã¾ã™")

                    proj_rows = ltv_table[ltv_table["äºˆæ¸¬"]].copy()
                    edit_df = proj_rows[["å®šæœŸå›æ•°", "ç¶™ç¶šç‡(%)", "å¹³å‡å˜ä¾¡(å††)"]].copy()

                    edited = st.data_editor(
                        edit_df,
                        key="ltv_editor",
                        disabled=["å®šæœŸå›æ•°"],
                        use_container_width=True,
                    )

                    if st.button("å†è¨ˆç®—", key="recalc_ltv"):
                        new_rates = {}
                        new_amounts = {}
                        for _, erow in edited.iterrows():
                            order_num = int(erow["å®šæœŸå›æ•°"].replace("å›ç›®", ""))
                            new_rates[order_num] = float(erow["ç¶™ç¶šç‡(%)"])
                            new_amounts[order_num] = float(erow["å¹³å‡å˜ä¾¡(å††)"])
                        st.session_state["proj_rates"] = new_rates
                        st.session_state["proj_amounts"] = new_amounts
                        st.rerun()

                st.markdown("")

                # ========== ã‚°ãƒ©ãƒ• ==========
                fig = make_subplots(specs=[[{"secondary_y": True}]])

                fig.add_trace(
                    go.Bar(
                        x=agg_table["å®šæœŸå›æ•°"],
                        y=agg_table["æ®‹å­˜ç‡(%)"],
                        name="æ®‹å­˜ç‡(%)",
                        marker_color="rgba(74, 144, 217, 0.7)",
                        text=agg_table["æ®‹å­˜ç‡(%)"].apply(lambda v: f"{v}%"),
                        textposition="outside",
                        textfont=dict(size=10),
                    ),
                    secondary_y=False,
                )

                if not ltv_table.empty:
                    fig.add_trace(
                        go.Scatter(
                            x=ltv_table["å®šæœŸå›æ•°"],
                            y=ltv_table["LTV(å††)"],
                            name="1å¹´LTV(å††)",
                            mode="lines+markers+text",
                            text=[f"Â¥{v:,}" for v in ltv_table["LTV(å††)"]],
                            textposition="top center",
                            textfont=dict(size=9),
                            line=dict(color="#E74C3C", width=2.5),
                            marker=dict(size=7),
                        ),
                        secondary_y=True,
                    )

                fig.update_layout(
                    title="æ®‹å­˜ç‡ & 1å¹´LTV æ¨ç§»",
                    xaxis_title="å®šæœŸå›æ•°",
                    height=420,
                    margin=dict(l=50, r=50, t=50, b=40),
                    legend=dict(orientation="h", y=1.12),
                )
                fig.update_yaxes(title_text="æ®‹å­˜ç‡ (%)", range=[0, 110], secondary_y=False)
                fig.update_yaxes(title_text="LTV (å††)", secondary_y=True)

                st.plotly_chart(fig, use_container_width=True)

                st.divider()
                render_download_buttons(agg_table, f"aggregate_{company_key}")


# =====================================================================
# æœˆåˆ¥ã‚³ãƒ›ãƒ¼ãƒˆã‚¿ãƒ–
# =====================================================================
with main_tab_monthly:
    if not filters["product_names"]:
        st.info("æ­£ç¢ºãªãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºã®ãŸã‚ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€Œå®šæœŸå•†å“åã€ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
    elif not st.button("è¡¨ç¤ºã™ã‚‹", key="btn_monthly", type="primary"):
        st.info("ãƒ•ã‚£ãƒ«ã‚¿ã‚’è¨­å®šã—ã¦ã€Œè¡¨ç¤ºã™ã‚‹ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    else:
        monthly_sql = build_cohort_sql(**filter_params)
        try:
            monthly_df = execute_query(client, monthly_sql)
        except Exception as e:
            st.error(f"BigQueryã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            monthly_df = pd.DataFrame()

        if monthly_df.empty:
            st.info("è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            summary_m = compute_summary_metrics(monthly_df)
            render_metrics([
                {"label": "æ–°è¦é¡§å®¢æ•° (åˆè¨ˆ)", "value": f"{summary_m['total_new_users']:,}"},
                {"label": "2å›ç›®å¹³å‡ç¶™ç¶šç‡", "value": f"{summary_m['avg_retention_2']}%"},
                {"label": "æœ€å¤æœˆ12å›ç›®æ®‹å­˜ç‡", "value": f"{summary_m['latest_12m_retention']}%"},
            ])

            st.divider()

            tab_heatmap, tab_line, tab_table, tab_schedule = st.tabs(
                ["ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—", "æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•", "ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«", "ç™ºé€æ—¥ç›®å®‰"]
            )

            # å•†å“å1ã¤é¸æŠæ™‚ã®ã¿ãƒã‚¹ã‚¯é©ç”¨
            _monthly_pn = filters["product_names"][0] if filters["product_names"] and len(filters["product_names"]) == 1 else None
            rate_matrix = build_retention_rate_matrix(monthly_df, data_cutoff_date, _monthly_pn)
            retention_table = build_retention_table(monthly_df, data_cutoff_date, _monthly_pn)

            with tab_heatmap:
                render_cohort_heatmap(rate_matrix)

            with tab_line:
                render_retention_line_chart(rate_matrix)

            with tab_table:
                st.dataframe(retention_table, use_container_width=True, hide_index=True)
                render_download_buttons(retention_table, f"cohort_{company_key}")

            with tab_schedule:
                selected_pn = filters["product_names"][0] if filters["product_names"] else None
                schedule = build_shipping_schedule(
                    cohort_months=monthly_df["cohort_month"].tolist(),
                    product_name=selected_pn,
                )
                if not schedule.empty:
                    st.dataframe(schedule, use_container_width=True, hide_index=True)
                else:
                    st.info("ç™ºé€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")


# =====================================================================
# ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡ã‚¿ãƒ– (å…¨ãƒãƒƒãƒ”ãƒ³ã‚°æ¨ªæ–­ã€ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨)
# =====================================================================
with main_tab_upsell:
    _all_mappings_raw = load_upsell_mappings()

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ãƒ•ã‚£ãƒ«ã‚¿ã§å¯¾è±¡ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’çµã‚Šè¾¼ã‚€
    _upsell_filter_pnames = filters.get("product_names")
    _upsell_filter_cats = filters.get("product_categories")
    if _upsell_filter_pnames:
        # å•†å“åãŒé¸æŠã•ã‚Œã¦ã„ã‚Œã° from_name ã§çµã‚‹
        _pname_set = set(_upsell_filter_pnames)
        all_mappings = [m for m in _all_mappings_raw if m.get("from_name") in _pname_set]
    elif _upsell_filter_cats:
        # å•†å“ã‚«ãƒ†ã‚´ãƒªãŒé¸æŠã•ã‚Œã¦ã„ã‚Œã°ã€ãã®ã‚«ãƒ†ã‚´ãƒªã«å±ã™ã‚‹å•†å“åã‚’å–å¾—ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿
        _table_ref = get_table_ref(company_key)
        _cat_product_names = fetch_filtered_options(
            client, _table_ref, Col.SUBSCRIPTION_PRODUCT_NAME,
            {Col.PRODUCT_CATEGORY: _upsell_filter_cats},
        )
        _cat_pname_set = set(_cat_product_names)
        all_mappings = [m for m in _all_mappings_raw if m.get("from_name") in _cat_pname_set]
    else:
        all_mappings = list(_all_mappings_raw)

    if not _all_mappings_raw:
        st.info("ã‚¢ãƒƒãƒ—ã‚»ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒã‚¹ã‚¿ç®¡ç†ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    elif not all_mappings:
        st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§é¸æŠä¸­ã®å•†å“ã«è©²å½“ã™ã‚‹ã‚¢ãƒƒãƒ—ã‚»ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    else:
        if st.button("è¡¨ç¤ºã™ã‚‹", key="btn_upsell", type="primary"):
            st.session_state["upsell_tab_shown"] = True
        if not st.session_state.get("upsell_tab_shown"):
            st.info("ã€Œè¡¨ç¤ºã™ã‚‹ã€ã‚’æŠ¼ã™ã¨ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡ã‚’è¨ˆç®—ã—ã¾ã™ã€‚")
        else:
            # from_name å˜ä½ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            _upsell_groups: dict[str, dict] = {}
            for m in all_mappings:
                fn = m.get("from_name", "")
                un = m.get("upsell_name", "")
                uun = m.get("upsell_upsell_name")
                if not fn or not un:
                    continue
                if fn not in _upsell_groups:
                    _upsell_groups[fn] = {"upsell_names": [], "upsell_upsell_names": []}
                if un not in _upsell_groups[fn]["upsell_names"]:
                    _upsell_groups[fn]["upsell_names"].append(un)
                if uun and uun not in _upsell_groups[fn]["upsell_upsell_names"]:
                    _upsell_groups[fn]["upsell_upsell_names"].append(uun)

            upsell_sub_agg, upsell_sub_monthly = st.tabs(["é€šç®—", "æœˆåˆ¥"])

            # ---------- é€šç®—ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡ ----------
            with upsell_sub_agg:
                for _gi, (from_name, group) in enumerate(_upsell_groups.items()):
                    with st.expander(f"ğŸ“¦ {from_name}", expanded=True):
                        # ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡: å„ upsell_name
                        for _ui, un in enumerate(group["upsell_names"]):
                            _render_upsell_pair(
                                client, company_key,
                                from_name, un,
                                "ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡",
                                date_from_str, date_to_str,
                                pair_key=f"agg_{_gi}_{_ui}",
                            )
                        # ã‚¢ãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡: å„ upsell_name Ã— å„ upsell_upsell_name
                        if group["upsell_upsell_names"]:
                            st.divider()
                            for _uui, uun in enumerate(group["upsell_upsell_names"]):
                                for _ui2, un in enumerate(group["upsell_names"]):
                                    _render_upsell_pair(
                                        client, company_key,
                                        un, uun,
                                        "ï½±ï½¯ï¾Œï¾Ÿï½±ï½¯ï¾Œï¾Ÿï½¾ï¾™ç‡",
                                        date_from_str, date_to_str,
                                        skip_if_no_normal=True,
                                        pair_key=f"agg_uu_{_gi}_{_uui}_{_ui2}",
                                    )

            # ---------- æœˆåˆ¥ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡ ----------
            with upsell_sub_monthly:
                for from_name, group in _upsell_groups.items():
                    with st.expander(f"ğŸ“¦ {from_name}", expanded=True):
                        for un in group["upsell_names"]:
                            _render_upsell_monthly(
                                client, company_key,
                                from_name, un,
                                "ã‚¢ãƒƒãƒ—ã‚»ãƒ«ç‡",
                                date_from_str, date_to_str,
                            )
                        if group["upsell_upsell_names"]:
                            st.divider()
                            for uun in group["upsell_upsell_names"]:
                                for un in group["upsell_names"]:
                                    _render_upsell_monthly(
                                        client, company_key,
                                        un, uun,
                                        "ï½±ï½¯ï¾Œï¾Ÿï½±ï½¯ï¾Œï¾Ÿï½¾ï¾™ç‡",
                                        date_from_str, date_to_str,
                                        skip_if_no_normal=True,
                                    )
